# -*- coding: utf-8 -*-
"""skin-allergy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mEIMZRRS3TJf8wHvvx_isqq77mI-1Kq
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive/Colab Notebooks"

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/Colab Notebooks

!kaggle datasets download -d shubhamgoel27/dermnet --force

!ls

!unzip \*.zip  && rm *.zip

!ls

import tensorflow as tf
import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from keras.preprocessing import image

import tensorflow as tf
import numpy as np
import pandas as pd
import os
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from keras.preprocessing import image

p=Path('./train')
dirs=p.glob('*')
image_data=[]
labels=[]
categories=[]
y_label=[]
for d in dirs:
    categories.append((str(d).split('/')[-1]))
    for img_path in d.glob('*.jpg'):
        img=image.load_img(img_path)
        gray=cv2.resize(np.float32(img),(80,80))
        img_array=image.img_to_array(gray)
        image_data.append(img_array)
        labels.append((str(d).split('/')[-1]))
for i in range(len(labels)):
    y_label.append(categories.index(labels[i]))

x=np.array(image_data)/255
y=np.array(y_label)

y

from sklearn.utils import shuffle
train_data,train_label=shuffle(x,y,random_state=0)
import pickle
with open('train_skin.pickle', 'wb') as f:
    pickle.dump([train_data, train_label], f)

plt.imshow(x[3487])
print(y[3487])

#backbone = tf.keras.applications.ResNet50(input_shape=(100,100,3),weights='imagenet', include_top=False)

backbone=tf.keras.applications.VGG16(
    include_top=False, 
    weights='imagenet',
    input_shape=(80, 80, 3)
)

backbone.trainable = False

learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=5, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=1e-7)

#ResNet50
input_shape=(80,80,3)
lr = 1e-5
epochs = 50
batch_size = 64

model = ResNet50(include_top=True,
                 weights= None,
                 input_tensor=None,
                 input_shape=input_shape,
                 pooling='avg',
                 classes=2)

model.compile(optimizer = Adam(lr) ,
              loss = "binary_crossentropy", 
              metrics=["accuracy"])

history = model.fit(x, y, validation_split=0.2,
                    epochs= epochs, batch_size= batch_size, verbose=2, 
                    callbacks=[learning_rate_reduction]
                   )

# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

model.summary()


model.fit(x, y,
          epochs=epochs, batch_size= epochs, verbose=0,
          callbacks=[learning_rate_reduction]
         )


checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1,
                                                 period=5)


model.save_weights(checkpoint_path.format(epoch=0))

try:
    latest = tf.train.latest_checkpoint(checkpoint_dir)
    model.load_weights(latest)
except:
    pass

history = model.fit(train_data,train_label,
                    epochs=20,
                    validation_split=0.2,
                    callbacks=[cp_callback])


pred_image=image.load_img('')
gray_pred=cv2.resize(np.float32(pred_image),(100,100))
pred_img_array=image.img_to_array(gray_pred)
pred_img_array=np.array(pred_img_array)/255
x_pred=pred_img_array.reshape((-1,100,100,3))

predicted_category=categories[np.argmax(model.predict([x_pred]))]
print(predicted_category)
